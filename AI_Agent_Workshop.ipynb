{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![Img](https://app.theheadstarter.com/static/hs-logo-opengraph.png)\n",
        "\n",
        "# Headstarter AI Agent Workshop\n",
        "\n",
        "#### **Skills: OpenAI, Groq, Llama, OpenRouter**\n",
        "\n",
        "## **To Get Started:**\n",
        "1. [Get your Groq API Key](https://console.groq.com/keys)\n",
        "2. [Get your OpenRouter API Key](https://openrouter.ai/settings/keys)\n",
        "3. [Get your OpenAI API Key](https://platform.openai.com/api-keys)\n",
        "\n",
        "\n",
        "### **Interesting Reads**\n",
        "- [Sam Altman's Blog Post: The Intelligence Age](https://ia.samaltman.com/)\n",
        "- [What LLMs cannot do](https://ehudreiter.com/2023/12/11/what-llms-cannot-do/)\n",
        "- [Chain of Thought Prompting](https://www.promptingguide.ai/techniques/cot)\n",
        "- [Why ChatGPT can't count the number of r's in the word strawberry](https://prompt.16x.engineer/blog/why-chatgpt-cant-count-rs-in-strawberry)\n",
        "\n",
        "\n",
        "## During the Workshop\n",
        "- [Any code shared during the workshop will be posted here](https://docs.google.com/document/d/1hPBJt_4Ihkj6v667fWxVjzwCMS4uBPdYlBLd2IqkxJ0/edit?usp=sharing)"
      ],
      "metadata": {
        "id": "nO8gHDbSAa4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install necessary libraries"
      ],
      "metadata": {
        "id": "Pt_SMGlvocqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install openai groq"
      ],
      "metadata": {
        "id": "eKvKwO8XAnPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up Groq, OpenRouter, & OpenAI clients"
      ],
      "metadata": {
        "id": "GjcgEeFaof1i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pHbjDU_L__Vd"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import json\n",
        "from groq import Groq\n",
        "import json\n",
        "from typing import List, Dict, Any, Callable\n",
        "import ast\n",
        "import io\n",
        "import sys\n",
        "\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "os.environ['GROQ_API_KEY'] = groq_api_key\n",
        "\n",
        "openrouter_api_key = userdata.get(\"OPENROUTER_API_KEY\")\n",
        "os.environ['OPENROUTER_API_KEY'] = openrouter_api_key\n",
        "\n",
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "groq_client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
        "\n",
        "openrouter_client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.getenv(\"OPENROUTER_API_KEY\")\n",
        ")\n",
        "\n",
        "openai_client = OpenAI(\n",
        "    base_url=\"https://api.openai.com/v1\",\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define functions to easily query and compare responses from OpenAI, Groq, and OpenRouter"
      ],
      "metadata": {
        "id": "AZF6uLpooj-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llm_response(client, prompt, openai_model=\"o1-preview\", json_mode=False):\n",
        "\n",
        "    if client == \"openai\":\n",
        "\n",
        "        kwargs = {\n",
        "            \"model\": openai_model,\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "        }\n",
        "\n",
        "        if json_mode:\n",
        "            kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "        response = openai_client.chat.completions.create(**kwargs)\n",
        "\n",
        "    elif client == \"groq\":\n",
        "\n",
        "        try:\n",
        "            models = [\"llama-3.1-8b-instant\", \"llama-3.1-70b-versatile\", \"llama3-70b-8192\", \"llama3-8b-8192\", \"gemma2-9b-it\"]\n",
        "\n",
        "            for model in models:\n",
        "\n",
        "                try:\n",
        "                    kwargs = {\n",
        "                        \"model\": model,\n",
        "                        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "                    }\n",
        "                    if json_mode:\n",
        "                        kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "                    response = groq_client.chat.completions.create(**kwargs)\n",
        "\n",
        "                    break\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error: {e}\")\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "            kwargs = {\n",
        "                \"model\": \"meta-llama/llama-3.1-8b-instruct:free\",\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "            }\n",
        "\n",
        "            if json_mode:\n",
        "                kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "            response = openrouter_client.chat.completions.create(**kwargs)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid client: {client}\")\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "def evaluate_responses(prompt, reasoning_prompt=False, openai_model=\"o1-preview\"):\n",
        "\n",
        "    if reasoning_prompt:\n",
        "        prompt = f\"{prompt}\\n\\n{reasoning_prompt}.\"\n",
        "\n",
        "    openai_response = get_llm_response(\"openai\", prompt, openai_model)\n",
        "    groq_response = get_llm_response(\"groq\", prompt)\n",
        "\n",
        "    print(f\"OpenAI Response: {openai_response}\")\n",
        "    print(f\"\\n\\nGroq Response: {groq_response}\")"
      ],
      "metadata": {
        "id": "BNYyDCNuAhbj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j9V03RXNBd6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wYaiRrHwbwMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mq1YkfFeCRl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zgyXx8M9C0RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent Architecture"
      ],
      "metadata": {
        "id": "0w8IlZq49PAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![](https://mermaid.ink/img/pako:eNqFUslugzAQ_ZWRz8kPILUVCZClVdQmqdTK5ODCFFDAjry0iSD_XmOTJodK5WS_Zd7M4JZkIkcSkEKyQwnbKOVgvzf6qlDCi0F52sF4fA-hJ0IaGi24aIRREBbItacn9LlmnKPced2kR7s1aiO5AmU-NFN71cG0fRLiALqUwhQlbAbi7F1TVyuia2RKXItFDo5pmOnqBo5dhgcDmKFlmEaY2oE6SOgvwHgO8REzM5B_2n1kxYsOZtSrxSUocfkzf5m5y5zGX6w27Cqau3IDOpTU8gR3kLBa2Y4W7QqP-jLywzDywtnesd_NLbISHSxpUnFWQ8jVt_0b8VFLll0Tl66TR-q3DLfaf3vaSoMukYxIg7JhVW4fQdvbUqJLbDAlgT3mTO5TkvKz1TG7ks2JZyTQ1j0i5pDb9UYVs2-nIcFnP-YFjfPKNjqA5x9CM8YW?type=png)](https://mermaid.live/edit#pako:eNqFUslugzAQ_ZWRz8kPILUVCZClVdQmqdTK5ODCFFDAjry0iSD_XmOTJodK5WS_Zd7M4JZkIkcSkEKyQwnbKOVgvzf6qlDCi0F52sF4fA-hJ0IaGi24aIRREBbItacn9LlmnKPced2kR7s1aiO5AmU-NFN71cG0fRLiALqUwhQlbAbi7F1TVyuia2RKXItFDo5pmOnqBo5dhgcDmKFlmEaY2oE6SOgvwHgO8REzM5B_2n1kxYsOZtSrxSUocfkzf5m5y5zGX6w27Cqau3IDOpTU8gR3kLBa2Y4W7QqP-jLywzDywtnesd_NLbISHSxpUnFWQ8jVt_0b8VFLll0Tl66TR-q3DLfaf3vaSoMukYxIg7JhVW4fQdvbUqJLbDAlgT3mTO5TkvKz1TG7ks2JZyTQ1j0i5pDb9UYVs2-nIcFnP-YFjfPKNjqA5x9CM8YW)\n",
        "\n",
        "\n",
        "![agent_architecture_v2](https://github.com/user-attachments/assets/a65b6db9-bef1-4579-aed3-01444ce40544)"
      ],
      "metadata": {
        "id": "RxpUp2KED9hh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To create our AI Agent, we will define the following functions:\n",
        "\n",
        "1. **Planner:** This function takes a user's query and breaks it down into smaller, manageable subtasks. It returns these subtasks as a list, where each one is either a reasoning task or a code generation task.\n",
        "\n",
        "2. **Reasoner:** This function provides reasoning on how to complete a specific subtask, considering both the overall query and the results of any previous subtasks. It returns a short explanation on how to proceed with the current subtask.\n",
        "\n",
        "3. **Actioner:** Based on the reasoning provided for a subtask, this function decides whether the next step requires generating code or more reasoning. It then returns the chosen action and any necessary details to perform it.\n",
        "\n",
        "4. **Evaluator:** This function checks if the result of the current subtask is reasonable and aligns with the overall goal. It returns an evaluation of the result and indicates whether the subtask needs to be retried.\n",
        "\n",
        "5. **generate_and_execute_code:** This function generates and executes Python code based on a given prompt and memory of previous steps. It returns both the generated code and its execution result.\n",
        "\n",
        "6. **executor:** Depending on the action decided by the \"actioner,\" this function either generates and executes code or returns reasoning. It handles the execution of tasks based on the action type.\n",
        "\n",
        "7. **final_answer_extractor:** After all subtasks are completed, this function gathers the results from previous steps to extract and provide the final answer to the user's query.\n",
        "\n",
        "8. **autonomous_agent:** This is the main function that coordinates the process of answering the user's query. It manages the entire sequence of planning, reasoning, action, evaluation, and final answer extraction to produce a complete response."
      ],
      "metadata": {
        "id": "goGb1KUVmVu1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BpZ1hRaiD-hS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MUMjfWwcK7ig"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BWjY2S2mxh3w"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1BFyiCpRxh6I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI o1-preview model getting trivial questions wrong\n",
        "\n",
        "- [Link to Reddit post](https://www.reddit.com/r/ChatGPT/comments/1ff9w7y/new_o1_still_fails_miserably_at_trivial_questions/)\n",
        "- Links to ChatGPT threads: [(1)](https://chatgpt.com/share/66f21757-db2c-8012-8b0a-11224aed0c29), [(2)](https://chatgpt.com/share/66e3c1e5-ae00-8007-8820-fee9eb61eae5)\n",
        "- [Improving reasoning in LLMs through thoughtful prompting](https://www.reddit.com/r/singularity/comments/1fdhs2m/did_i_just_fix_the_data_overfitting_problem_in/?share_id=6DsDLJUu1qEx_bsqFDC8a&utm_content=2&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1)\n"
      ],
      "metadata": {
        "id": "MrSeaoPqq1qY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rM4bcny1JliP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vXAPpgkpK7lG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RjKgUDihI4lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5JNPSaGWKF5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ciZ_VUNNJRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Vf9avjo8-fK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}